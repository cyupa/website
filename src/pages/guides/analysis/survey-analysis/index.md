---
date: "2018-04-11"
path: "/guides/survey-analysis"
title: "How to analyze open-ended survey questions"
weight: 3
---

![Photo of a laptop sitting on a sofa showing statistics and charts](./laptop.jpg)

Designing surveys is like making pasta. Anyone can make okay pasta, but it takes effort, patience, and skill to makeÂ amazingÂ pasta. [Likewise for surveys](https://medium.com/research-things/on-surveys-5a73dda5e9a0).

The quality of your results depends on the questions you ask, the order you ask them in, and the type of people who complete your survey. Not to mention how many people respond and its statistical significance. However, this article is about analysis, not survey design, so from here on weâ€™ll assume youâ€™re making great surveys and focus on analyzing the data you collect.

Analysis is what you do with survey results once youâ€™ve collected them. It can be as rigorous or relaxed as you like. If youâ€™re just looking for directional indicators, thereâ€™s no need to spend weeks analyzing your data. If your survey is mostly comprised of quantitative questions (e.g. boolean yes / no, multiple choice, Likert scale) then your analysis should be a fairly straightforward game of crunching numbers, and there are plenty of articles on the web that talk about analyzing quantitative data with formulas in spreadsheets.

**See also: [The difference between qualitative & quantitative research](/guides/qual-quant)**

Analysis gets a bit more complicated if youâ€™re creating surveysÂ with open-ended questions. Analyzing this sort of data is called qualitative data analysis or QDA for short. Itâ€™s usually a cumbersome process involving some combination of clunky analysis software, sticky notes, spreadsheets, Microsoft Word, and lots of coffee. For lots of researchers, itâ€™s kind of the least favorite part of research.

In this article weâ€™ll give a basic overview of how to analyze qualitative data in laymanâ€™s terms, and offer a few suggestions on how to get better insights from open-ended survey questions, while making your life easier.

## Step one â€“ understand your goals

![A photo of a single sticky note with a light bulb sketch](./why.jpg)

If most people told you theyâ€™d rather fight one horse-sized duck instead of 100 duck-sized horses, how would that information change your product? Your research (and therefore analysis) should have a practical purpose. Start by asking yourself what youâ€™re trying to understand, and what you would change about your product / service based on some hypothetical findings.

In the case of user research, generally researchers are looking for user pain points and trying to understand what would solve them. More practically, they might be tasked with understanding why customers churn or donâ€™t purchase in the first place. Market research is a bit different in that researchers want to understand broader trends and discover opportunities.

Other uses for qualitative data analysis include analysis of competitors, industry trends, customer interview transcripts, user testing notes, and of course analyzing survey results.

Each nugget of wisdom you learn is usually called anÂ â€œinsightâ€. The general idea is to start with a bunch of raw data and end with these actionable insights that you can share with your team, stakeholders, or clients.

## The art of coding (no, not that kind)

The actual method of turning qualitative data into insights is a technique calledÂ [coding](<https://en.wikipedia.org/wiki/Coding_(social_sciences)>). Personally I prefer the simpler termÂ â€˜taggingâ€™ instead. Basically, tagging qualitative data involves these five steps:

1.  Look through chunks of qualitative data (text, images, or video)
1.  Identify repeating themes (e.g. pain points, problems, or opportunities)
1.  Tag them with aÂ â€˜codeâ€™ to make them searchable and countable
1.  Evolve your codes, merging them and breaking them down
1.  End up with a bunch of themes and an idea of their frequency.

Hereâ€™s an example. Letâ€™s say youâ€™re a researcher working for a large supermarket chain that offers home deliveries. After someoneâ€™s order is delivered, theyâ€™re emailed a survey which asks them a few open-ended questions like â€œHow was your delivery experience?â€ and â€œWhat can we improve for next time?â€

Imagine this is one of the answers to the first question:

> The delivery driver was two hours late. We had to leave the house to drop our son off at water polo practice. The driver arrived while we werenâ€™t home, so he called us, but had an unknown number. We only answered after he called a few times. He refused to leave the groceries without us signing for him, so we had to rush home to meet him. Totally inconvenient!

You might notice there are a few distinct problems happening here.

1.  The driver was not on time.
1.  People have lives so canâ€™t always wait if the driver is late.
1.  Company cellphones block caller ID.
1.  Policy mandates the groceries canâ€™t be left without a signature.

The survey responses are probably stored in the survey software like SurveyMonkey, Wufoo, Typeform, Qualtrics, and so on. Most survey software is built to help you collect a lot of data, but they usually have few features to help you make sense of the data youâ€™ve collected. So you download the data to analyze in a spreadsheet, on paper, or in another analysis tool.

You might start off by tagging the entry withÂ â€œLate driverâ€ and â€œCompany policyâ€. Perhaps youâ€™d break downÂ â€œCompany policyâ€ into more specific tags likeÂ â€œDriver caller IDâ€ andÂ â€œSignature requiredâ€. Youâ€™ve just started creating a system of tags, which you can then re-use when you hear these problems again from another customer.

Eventually you might discover that the caller ID problem isnâ€™t a common complaint (only 5 people out of 2000 mentioned that in the past month), but late drivers certainly is (340 / 2000 in the past month). So you report the findings to the head of logistics, who starts to research the core reasons why her drivers are late, and the whole cycle repeats itself. Everyoneâ€™s doing research all the time to different degrees to understand problems and weigh them up against each other in terms of pain, frequency, and cost.

## Youâ€™re already good at this

![Photo of a pattern on a building](./pattern.jpg)

Qualitative research is fundamentally about understanding people and recognizing patterns. Thankfully, pattern-recognition is something humans are innately extremely good at [thanks to the expansion of our cerebral cortex](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4141622/).

Pattern-recognition allows animals to do cool stuff like create cognitive maps of the environment, distinguish individuals and their emotional state based on facial features, and use gestures to communicate with others. The cognitive repertoire of humans far exceeds that of animals, and gives humans creativity and invention, spoken and written languages, reasoning and rapid decision-making, imagination and mental time travel, and magical thinking & fantasy.

So while youâ€™re slogging through tagging your survey responses, just remember that itâ€™s much easier than it could be thanks to your innate human ability to recognize patterns in things! Thank you, cerebral cortex ğŸ‘

## Canâ€™t machines do this for us?

![Photo of a laptop with a â€˜Matrixâ€™ style wallpaper](./matrix.jpg)

Kind of. There are two broad techniques to automate qualitative data analysis. Both of them work in certain scenarios but are not without constraints.

#### Automated rules

Thereâ€™s a lot of research software that lets you set up automatedÂ â€˜rulesâ€™ based on keywords. These rules essentially parse incoming data and tag it with something automatically. For example you might set up a rule to tag anything containing the wordÂ â€œlateâ€ with â€œLate driverâ€.

This works okay, however itâ€™s prone to mistakes because the context is missingâ€”the word â€œlateâ€ alone could be used in a variety of contextsâ€”and there are many ways to describe a late driver without using that particular keyword, for example â€œnot on timeâ€ or â€œdelayed deliveryâ€.

#### Machine learning

Thereâ€™s a lot of emerging technology around machine learning where youâ€™d train an Artificial Intelligence (AI) to recognize patterns and tag them for you.

These systems tend to be more accurate than automated rules, but only with a lot of data to train the system in the first place. Like tens of thousands of data points. Not many smallâ€“medium companies have that much training data. As well as the training data requirement, machine learning systems are difficult to set up, so most user researchers outside of huge companies or academia ultimately resort to analyzing qualitative data manually.

---

#### Liked this article?

[Check out our other research guides](/guides) or [learn more](/) about how Dovetail can help you with customer feedback and user research analysis.
